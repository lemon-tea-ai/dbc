{
    "posts": [
        {
            "title": "U-Net Skip Connections in PyTorch",
            "content": "# Overview\nSkip connections in U-Net architecture allow direct information flow from encoder layers to decoder layers, preserving fine-grained details and preventing vanishing gradients.\n\n## Code Implementation\n```python\ndef forward(self, x, skip):\n    x = torch.cat((x, skip), 1)  # Concatenate along channel dimension\n    x = self.model(x)\n    return x\n```\n\n## Key Components\n\n### Parameters\n- `x`: Features from previous decoder layer\n- `skip`: Features from corresponding encoder layer\n- `dim=1`: Channel dimension for concatenation\n\n### Tensor Dimensions (NCHW Format)\n- dim=0: Batch size\n- dim=1: Channels\n- dim=2: Height\n- dim=3: Width\n\n### Example Shapes\n```python\nx = torch.zeros(2, 64, 28, 28)      # [batch, channels, height, width]\nskip = torch.ones(2, 64, 28, 28)    \nresult = torch.cat((x, skip), 1)     # Result: [2, 128, 28, 28]\n```\n\n## Benefits\n1. Preserves fine-grained spatial details\n2. Prevents vanishing gradients\n3. Enables better gradient flow\n4. Combines high-level semantic info (decoder) with low-level details (encoder)\n\n## Common Use Cases\n- Image segmentation\n- Image generation\n- Feature reconstruction tasks\n- Any task requiring both high-level and low-level feature preservation",
            "category": "Deep Learning",
            "date": "Dec 23, 2024"
        },
        {
        "title": "Understanding Context U-Net in Diffusion Models",
        "date": "Dec 23, 2024",
        "category": "Deep Learning",
        "content": "# Understanding Context U-Net in Diffusion Models\n\n## 1. Architecture Overview\nThe Context U-Net combines three key elements:\n- Standard U-Net architecture (for spatial features)\n- Time embeddings (for diffusion steps)\n- Context embeddings (for conditional generation)\n\n## 2. Key Components\n\n### Time Embedding\n```python\n# Input: timestep t (scalar)\nself.timeembed1 = EmbedFC(1, 2*n_feat) # larger embedding\nself.timeembed2 = EmbedFC(1, n_feat) # smaller embedding\n\n# Output shapes:\ntemb1: [batch, 2*n_feat, 1, 1]\ntemb2: [batch, n_feat, 1, 1]\n```\n\nPurpose: Helps model understand noise level at each diffusion step\n\n### Context Embedding\n```python\n# Input: class label -> one-hot vector\nself.contextembed1 = EmbedFC(n_classes, 2*n_feat)\nself.contextembed2 = EmbedFC(n_classes, n_feat)\n\n# Output shapes:\ncemb1: [batch, 2*n_feat, 1, 1]\ncemb2: [batch, n_feat, 1, 1]\n```\n\n### Feature Modulation\n```python\n# Combining embeddings with features\nup2 = self.up1(cemb1*up1 + temb1, down2)\nup3 = self.up2(cemb2*up2 + temb2, down1)\n```\n\n## 3. How Feature Modulation Works\n\n### Channel-wise Multiplication\n```python\n# Shapes\nup1: [batch, channels, height, width] # e.g., [256, 512, 7, 7]\ncemb1: [batch, channels, 1, 1] # e.g., [256, 512, 1, 1]\n```"
        },
        {
        "title": "Understanding Generator Architecture in GANs",
        "date": "Dec 13, 2024",
        "category": "Deep Learning",
        "content": "# Generator Architecture in GANs\n\n## Core Concepts\n- Transforms random noise (typically 100d) into images\n- Uses progressive expansion of dimensions\n- Works in both RGB and grayscale domains\n\n## Architecture Components\n1. Input Layer\n - Takes random noise vector (dim_z=100)\n - Optional conditional input (e.g., class labels)\n\n2. Hidden Layers\n - Progressive expansion: [100] → [512] → [256] → [128]\n - Each block contains:\n - Linear transformation\n - ReLU activation\n - BatchNorm for stability\n - Dropout for regularization\n\n3. Output Layer\n - Produces final image dimensions\n - For MNIST: 784 (28x28x1)\n - For RGB: width × height × 3\n - Uses Tanh activation\n\n## Key Design Principles\n- Reversed channel ordering for better information flow\n- Larger to smaller feature dimensions\n- Avoids information bottlenecks\n- Balances between capacity and efficiency\n\n## Common Variations\n- Convolutional vs Linear layers\n- Different image sizes and channels\n- Conditional vs unconditional generation"
        },
        {
        "title": "GAN Discriminator Implementation Notes",
        "date": "Dec 13, 2024",
        "category": "Deep Learning",
        "content": "# GAN Discriminator Implementation Notes\n\n## 1. Binary Cross Entropy Loss (nn.BCELoss)\n- Used for binary classification (real/fake)\n- Takes inputs between 0 and 1\n- Formula: -[y * log(x) + (1 - y) * log(1 - x)]\n- Used in both discriminator and generator training\n\n## 2. Backward Pass and Optimization\n```python\nd_loss.backward() # Computes gradients\nd_optimizer.step() # Updates parameters\n```\n\n## 3. Tensor Reshaping Operations\n### view() Function\n```python\nx = x.view(x.size(0), 784) # Reshapes input to [batch_size, 784]\nlabel = label.view(-1, 1) # Reshapes to column vector\n```\n\n### One-hot Encoding\n```python\nlabel_onehot = torch.zeros(x.size(0), 10)\nlabel_onehot.scatter_(1, label.view(-1, 1), 1)\n```\n\n## 4. Dropout Layer\n```python\nnn.Dropout(0.3) # 30% dropout rate\n```"
        },
        {
        "title": "PixelCNN Architecture Implementation Notes",
        "date": "Dec 12, 2024",
        "category": "Deep Learning",
        "content": "# PixelCNN Implementation Key Concepts\n\n## 1. MaskedConv2d Layer\n- Inherits from nn.Conv2d\n- Implements autoregressive property through masking\n- Two mask types:\n - Type A: Masks current and future pixels\n - Type B: Masks only future pixels\n\n### Masking Implementation\n```python\nmask[:, :, height//2, width//2:] = 0 # Mask current and right pixels\nmask[:, :, height//2 + 1:, :] = 0 # Mask all pixels below\n```\n\n## 2. Convolution Concepts\n### Shape Calculations\n- Input shape: (batch_size, channels, height, width)\n- Output shape = ((Input - Kernel + 2Padding) / Stride) + 1\n- Example: 28x28 input with 7x7 kernel, padding=3:\n```python\n((28 - 7 + 2*3) / 1) + 1 = 28 # Maintains spatial dimensions\n```\n\n## 3. Network Architecture\n### Layer Organization\n```python\nnn.ModuleList([\n    # Input layer (Type A mask)\n    MaskedConv2d(mask_type='A', ...),\n    nn.ReLU(),\n    \n    # Middle layers (Type B mask)\n    *[nn.Sequential(\n        MaskedConv2d(mask_type='B', ...),\n        nn.ReLU()\n    ) for _ in range(7)],\n    \n    # Output layer\n    MaskedConv2d(mask_type='B', ...),\n    nn.Sigmoid()\n])\n```"
        },
        {
            "title": "VAE Architecture Implementation Summary",
            "date": "Dec 12, 2024",
            "category": "Deep Learning",
            "content": "# Summary of VAE Implementation Notebook\n\nThis notebook implements and compares different autoencoder architectures on the MNIST dataset and a 3D torus point cloud. Here's a breakdown of the main components:\n\n## 1. Preliminary Setup\n- Imports required libraries (PyTorch, matplotlib, numpy)\n- Sets up MNIST data loading with batch size 256\n- Implements common utility functions for training and evaluation\n\n## 2. Autoencoder (AE) Implementation\n- Basic autoencoder architecture with configurable hidden dimensions\n- Uses MSE loss for reconstruction\n- Includes visualization functions for original vs reconstructed images\n\n## 3. Variational Autoencoder (VAE) Implementation\n- Implements VAE with reparameterization trick\n- Two loss function implementations:\n  - Stochastic Gradient Variational Bayes (SGVB) Estimator\n  - KL Divergence without Estimation\n- Compares both loss functions empirically\n- Includes training and evaluation on MNIST\n\n## 4. Torus Point Cloud VAE\n- Generates 3D torus point cloud data\n- Implements custom PositionalEncoding3D for 3D coordinates\n- Creates specialized VAE architecture for point cloud data\n- Includes visualization for:\n  - Original point cloud\n  - Reconstructed point cloud"
        },
        {
            "title": "Deep Learning Core Concepts: ResidualBlock, Vanishing Gradient, and BatchNorm",
            "date": "Dec 12, 2024",
            "category": "Deep Learning",
            "content": "# Deep Learning Concepts Q&A\n\n## ResidualBlock\n**Q: What is ResidualBlock?**\n\nA ResidualBlock is a neural network component that helps solve the degradation problem in deep networks through skip connections.\n\nKey points:\n- Uses skip connections (`x + block(x)`)\n- Helps prevent vanishing gradients\n- Makes it easier to preserve important features\n- Improves training stability\n\n```python\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.block = torch.nn.Sequential(\n            torch.nn.Linear(dim, dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(dim, dim)\n        )\n    \n    def forward(self, x):\n        return x + self.block(x)\n```\n\n## Vanishing Gradient\n**Q: What is vanishing gradient?**\n\nVanishing gradient is when gradients become extremely small during backpropagation in deep networks.\n\nSolutions include:\n- Using ReLU activation instead of Sigmoid/Tanh\n- Adding residual connections\n- Implementing batch normalization\n- Proper weight initialization\n\n## Batch Normalization\n**Q: What is batch norm?**\n\nBatchNorm normalizes intermediate layer outputs during training to improve stability and speed.\n\nBenefits:\n- Speeds up training\n- Provides regularization\n- Makes networks more robust\n- Allows higher learning rates\n\n```python\nclass SimpleNetwork(torch.nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.bn_network = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, 100),\n            torch.nn.BatchNorm1d(100),\n            torch.nn.ReLU()\n        )\n```\n\n## Multiple ResidualBlocks\n**Q: Why use two residual blocks?**\n\nUsing multiple ResidualBlocks provides:\n- Increased model capacity\n- Hierarchical feature learning\n- Better transformation capabilities\n- Maintained gradient flow through skip connections\n\nThe number of blocks is a trade-off between model complexity and performance.",
            "tags": ["deep learning", "neural networks", "ResidualBlock", "BatchNorm", "vanishing gradient"]
        },
        {
            "title": "VAE Architecture Implementation Summary",
            "date": "Dec 11, 2024",
            "category": "Deep Learning",
            "content": "# VAE Architecture Implementation Summary\n\n## Key Differences between VAE and Regular Autoencoder\n\n1. **Encoder Output**: \n   - Regular AE: Outputs direct encoding\n   - VAE: Outputs parameters (mean and logvar) for a probability distribution\n   - Final layer size is doubled to accommodate both mean and logvar\n\n2. **Latent Space**:\n   - Regular AE: Deterministic encoding\n   - VAE: Probabilistic encoding (samples from distribution defined by mean/logvar)\n   - `z_size = hidden_dims[-1] // 2` (half of final hidden layer)\n\n## Architecture Dimensions Example\n\n```python\n# Given:\ninput_dim = 784  # (28x28 MNIST image)\nhidden_dims = [128, 64, 36, 18]\nz_size = 18 // 2 = 9  # (latent dimension)\n\n# Flow:\nEncoder: 784 → 128 → 64 → 36 → 18 (mean/logvar)\nSampling: 18 → 9 (latent space)\nDecoder: 9 → 36 → 64 → 128 → 784\n```\n\n## Important Code Components\n\n```python\n# Encoder dimensions\nencoder_dims = [input_dim] + hidden_dims\n# [784, 128, 64, 36, 18]\n\n# Decoder dimensions\ndecoder_dims = [self.z_size] + hidden_dims[:-1][::-1] + [decode_dim]\n# [9, 36, 64, 128, 784]\n```\n\n## Why Skip Last Hidden Dimension in Decoder?\n\n- Last encoder layer (18) outputs mean and logvar\n- After sampling, latent dimension is halved (9)\n- Decoder must mirror encoder structure without the final encoder layer\n- Creates symmetric architecture for better reconstruction\n\n## Key Implementation Points\n\n1. Encoder outputs twice the latent dimension (for mean and logvar)\n2. Decoder starts from sampled latent space (z_size)\n3. Architecture should be symmetric (except for latent space)\n4. No activation on final layers of both encoder and decoder\n5. Optional sigmoid at decoder output"
        },
        {
            "title": "Understanding VAE's Reparameterization Trick",
            "date": "Dec 11, 2024",
            "category": "Deep Learning",
            "content": "# Understanding VAE's Reparameterization Trick\n\n## 1. Purpose\nThe reparameterization trick in VAEs allows for backpropagation through a random sampling process, making the sampling process differentiable and enabling end-to-end training.\n\n## 2. Implementation\n```python\ndef reparameterize(self, mean, logvar, n_samples_per_z=1):\n    std = torch.exp(0.5 * logvar)  # Convert logvar to standard deviation\n    eps = torch.randn_like(std)     # Sample from standard normal N(0,1)\n    z = mean + eps * std            # Transform to desired distribution\n    return z\n```\n\n## 3. Key Components\n\n### A. Log Variance (logvar)\n- Used instead of variance (σ²) for several reasons:\n  - **Numerical Stability**: Handles very small/large variances better\n  - **Optimization Benefits**: Better gradient behavior\n  - **Unconstrained Optimization**: Can be any real number (-∞ to +∞)\n  - **Clean KL Divergence**: Simpler mathematical formulation\n\n### B. Converting logvar to Standard Deviation\n```\nGiven: logvar = log(σ²)\nTo get σ (standard deviation):\n1. logvar = log(σ²)\n2. exp(logvar) = σ²\n3. σ = √(exp(logvar)) = exp(logvar/2)\n```\n\n### C. Random Sampling (torch.randn_like)\n```python\neps = torch.randn_like(std)\n```\n\n- Generates random samples from N(0,1)\n- Matches input tensor's shape\n- Properties:\n  - Mean ≈ 0\n  - Standard deviation ≈ 1\n  - Different random samples each forward pass\n\n## 4. Mathematical Process\n1. Start with encoded mean and logvar\n2. Convert logvar to standard deviation\n3. Sample random noise from N(0,1)\n4. Transform noise to desired distribution: z = μ + σ * ε\n\n## 5. Example\n```python\n# Sampling from N(1, 4) distribution\nmean = torch.ones(3)     # [1, 1, 1]\nlogvar = torch.log(torch.tensor([4.0])).expand(3)\nstd = torch.exp(0.5 * logvar)  # = 2\neps = torch.randn_like(std)    # ~ N(0,1)\nz = mean + eps * std           # ~ N(1,4)\n```\n\n## 6. Benefits\n- Enables backpropagation\n- Maintains differentiability\n- Allows learning meaningful latent representations\n- Numerically stable\n- Computationally efficient"
        }
    ]
}